{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your file list\n",
    "import os\n",
    "os.chdir('///') ### change this! \n",
    "files = os.listdir('.')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    data = pd.read_csv(file)\n",
    "    print(file)\n",
    "    print(data.shape)\n",
    "    \n",
    "    filename = file.split('.')[0]+'_pron.csv'\n",
    "    pron = pronoun_analysis(file, filename)\n",
    "    #print('after prons: '+ str(pron.shape))\n",
    "    #print(filename)\n",
    "    \n",
    "    filename = file.split('.')[0]+'_time.csv'\n",
    "    time = time_analysis(file, filename)\n",
    "    #print('after tine: '+ str(time.shape))\n",
    "    #print(filename)\n",
    "    \n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lists(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    content_lists = []\n",
    "    #for i in (df['title'] + ' '+df['text_context']):\n",
    "    for i in df['post']:\n",
    "        i = i.replace('\\n', '')\n",
    "        content_lists.append(i.lower())\n",
    "    return content_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Features & Sentimental Analysis\n",
    "\n",
    "- LIWCalike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(\"quanteda.dictionaries\")\n",
    "library(\"corpus\")\n",
    "\n",
    "\n",
    "file = read.csv(\"/Users/midan/Downloads/final/final_all_4.csv\")  # here you should change yours\n",
    "file[2,7] # title + text\n",
    "\n",
    "contents = c(file[1,7])\n",
    "for (i in 2:nrow(file)){\n",
    "  temp = paste(file[i,7]) \n",
    "  contents = append(contents,temp, after = i-1)\n",
    "}\n",
    "\n",
    "\n",
    "nrow(file)\n",
    "length(output_lsd)\n",
    "length(contents)\n",
    "output_lsd <- liwcalike(contents, dictionary = data_dictionary_NRC)\n",
    "write.csv(output_lsd, '/Users/midan/Downloads/final/final_all_4_LIWCalike.csv')  # here you should change yours\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pronouns analysis & Time oriented analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pronons analysis\n",
    "def pronoun_analysis(filename,filename2):\n",
    "    content_lists = make_lists(filename)\n",
    "    pers1 = ['i ', 'my', 'me', 'mine', \"i'm\", \"i'd\", \"i've\", \"i'll\", 'I ']\n",
    "    pers1_p = ['we', 'our', 'us', 'ours', \"we're\", \"we'd\", \"we'll\"]\n",
    "    pers2 = ['you', 'u', 'yours', 'your', 'ur', \"you'll\", \"you've\", \"you'd\"]\n",
    "    pers3 = ['he', 'she', 'him', 'his', 'hers', 'her', \"she's\", \"she'd\", \"she'll\", \"he's\", \"he'd\", \"he'll\", \n",
    "        \"she've\", \"he've\"]\n",
    "    pers3_p = ['them', 'their','they', 'theirs',  \"they're\", \"they'd\", \"they'll\",  \"they've\"]\n",
    "    pers0 = ['it', 'that',  'this', 'its', \"it's\", \"that's\", \"these\", \"those\"]\n",
    "    \n",
    "    list1 = []\n",
    "    list1_p = []\n",
    "    list2 = []\n",
    "    list3 = []\n",
    "    list3_p = []\n",
    "    list0 = []\n",
    "    cnt = 0\n",
    "    for content in content_lists:\n",
    "        cnt+=1\n",
    "        if type(content) == float: \n",
    "            print(cnt)\n",
    "            print(content)\n",
    "            temp = content_lists[cnt]\n",
    "            wc = len(temp)\n",
    "            count1 = 0 # first person singular\n",
    "            count1_p = 0 # first person plural\n",
    "            count2 = 0 # second person\n",
    "            count3 = 0 # third person singular\n",
    "            count3_p = 0 # third person plural\n",
    "            count4 = 0 # impersonal\n",
    "            \n",
    "            for c in temp:\n",
    "                for pers in pers1: \n",
    "                    if pers == c: count1+=1\n",
    "                for pers in pers2: \n",
    "                    if pers == c: count2+=1\n",
    "                for pers in pers3: \n",
    "                    if pers == c: count3+=1\n",
    "                for pers in pers0: \n",
    "                    if pers == c: count4+=1\n",
    "                for pers in pers1_p: \n",
    "                    if pers == c: count1_p+=1\n",
    "                for pers in per3_p1: \n",
    "                    if pers == c: count3_p+=1\n",
    "        else:\n",
    "            content = content.split()\n",
    "            wc = len(content)\n",
    "            count1 = 0 # first person singular\n",
    "            count1_p = 0 # first person plural\n",
    "            count2 = 0 # second person\n",
    "            count3 = 0 # third person singular\n",
    "            count3_p = 0 # third person plural\n",
    "            count4 = 0 # impersonal\n",
    "            for c in content:\n",
    "                for pers in pers1: \n",
    "                    if pers == c: count1+=1\n",
    "                for pers in pers2: \n",
    "                    if pers == c: count2+=1\n",
    "                for pers in pers3: \n",
    "                    if pers == c: count3+=1\n",
    "                for pers in pers0: \n",
    "                    if pers == c: count4+=1\n",
    "                for pers in pers1_p: \n",
    "                    if pers == c: count1_p+=1\n",
    "                for pers in pers3_p: \n",
    "                    if pers == c: count3_p+=1\n",
    "            list1.append(count1/wc)\n",
    "            list1_p.append(count1_p/wc)\n",
    "            list2.append(count2/wc)\n",
    "            list3.append(count3/wc)\n",
    "            list3_p.append(count3_p/wc)\n",
    "            list0.append(count4/wc)\n",
    "    result = pd.DataFrame({\n",
    "        '1pers': list1, '1pers_plural': list1_p, '2pers': list2, '3pers': list3,'3pers_plural': list3_p, 'nonpers': list0, \n",
    "    })\n",
    "    result.to_csv(filename2, index = False)\n",
    "    #print('done!')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_analysis(filename, filename2):   \n",
    "    content_lists = make_lists(filename)\n",
    "    past =['ago', 'did', 'before', 'yesterday', 'last']\n",
    "    present = ['present', 'now', 'today', 'this']\n",
    "    future = ['future', 'may', 'will', 'soon', 'next', 'gonna']\n",
    "    vb_pr = ['VBP', 'VBZ', 'VB']\n",
    "    \n",
    "    future_s = []\n",
    "    present_s = []\n",
    "    past_s = []\n",
    "    for j in range(len(content_lists)):\n",
    "        #print(j)\n",
    "        content = content_lists[j]\n",
    "        if type(content)==float: f_s, pr_s, p_s = -1, -1, -1\n",
    "        else:\n",
    "            temp_text = nltk.word_tokenize(content)\n",
    "            f_s = 0 # future\n",
    "            pr_s = 0 # present\n",
    "            p_s = 0 #past\n",
    "            for t in temp_text:\n",
    "                if t in future: f_s +=1\n",
    "                if t in present: pr_s +=1\n",
    "                if t in past: p_s +=1\n",
    "            temp_tag = nltk.pos_tag(temp_text)\n",
    "            for i in range(len(temp_tag)):\n",
    "                if temp_tag[i][1] in vb_pr: pr_s +=1\n",
    "                elif temp_tag[i][1] == 'VBD': p_s +=1\n",
    "        future_s.append(f_s)\n",
    "        present_s.append(pr_s)\n",
    "        past_s.append(p_s)\n",
    "    result = pd.DataFrame({\n",
    "       'past':past_s, 'present': present_s ,'future': future_s\n",
    "    })\n",
    "    result.to_csv(filename2, index = False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('/Users/midan/Downloads/final/')\n",
    "files = os.listdir()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['final_all_1', 'final_all_2', 'final_all_3', 'final_all_4', 'final_all_5']:\n",
    "    df = pd.read_csv(i+'.csv')\n",
    "    pron = pd.read_csv(i+'_pron.csv')\n",
    "    time = pd.read_csv(i+'_time.csv')\n",
    "    liwc = pd.read_csv(i+'_LIWCalike.csv')\n",
    "    liwc = liwc[['WPS', 'WC', 'Sixltr','anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative',\n",
    "       'positive', 'sadness', 'surprise', 'trust']]\n",
    "    result = pd.concat([df, liwc, pron, time])\n",
    "    print(result.shape)\n",
    "    result.to_csv('LIWC_'+i+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
